ğŸ“Š Data Engineering Project: Orders & Weather Analytics Pipeline

ğŸ§  Project Overview

This project implements an end-to-end data engineering pipeline that integrates e-commerce order data with external weather data to enable time-based analytical insights.

The pipeline follows modern data engineering best practices, including layered data modeling, orchestration, cloud storage, and analytics-ready marts.

â¸»

ğŸ—ï¸ Architecture Overview

External API â†’ S3 â†’ Snowflake (RAW) â†’ dbt (STG â†’ PREP â†’ MART)

Orchestration & Infrastructure

Docker + Docker Compose
        â†“
Apache Airflow (Webserver + Scheduler)
        â†“
PostgreSQL (Airflow metadata DB)

Technologies Used
	â€¢	Python â€“ API ingestion and data processing
	â€¢	AWS S3 â€“ cloud object storage
	â€¢	Snowflake â€“ cloud data warehouse
	â€¢	dbt â€“ data transformation and modeling
	â€¢	Apache Airflow â€“ orchestration and scheduling
	â€¢	Docker & Docker Compose â€“ containerized infrastructure
	â€¢	PostgreSQL â€“ Airflow metadata database
	â€¢	SQL â€“ analytical transformations

â¸»

ğŸ“¥ Data Sources
	1.	E-commerce Orders Dataset
	â€¢	Orders, customers, delivery timestamps
	â€¢	Granularity: order-level events
	2.	Weather API (Open-Meteo)
	â€¢	Hourly weather data (temperature, humidity, precipitation, wind)
	â€¢	Location: Rio de Janeiro
	â€¢	Granularity: hourly

â¸»

ğŸ§± Data Modeling Strategy

The project follows a layered modeling approach:

ğŸ”¹ RAW Layer
	â€¢	Stores data as ingested
	â€¢	Minimal transformation
	â€¢	Schema mirrors source systems

ğŸ”¹ STAGING (STG)
	â€¢	Type casting
	â€¢	Column renaming
	â€¢	Timestamp normalization
	â€¢	No business logic

ğŸ”¹ PREP Layer
	â€¢	Joins between orders, customers, and weather
	â€¢	Alignment on hour-level grain using date_trunc('hour')
	â€¢	Clean, analytics-ready intermediate models

ğŸ”¹ MART Layer
	â€¢	Business-focused datasets
	â€¢	Aggregations (hourly / daily)
	â€¢	Designed for BI tools and analysts

â¸»

â±ï¸ Time Handling & Grain Design

A key challenge addressed in the project is time alignment across heterogeneous data sources.
	â€¢	All joins are performed using hourly timestamps
	â€¢	date_trunc('hour', timestamp) is used to ensure consistent grain
	â€¢	Avoided joining on TIME or numeric hour values
	â€¢	Ensured stable and reproducible analytics

â¸»

ğŸ” Orchestration (Airflow)
	â€¢	Airflow DAG schedules daily weather ingestion
	â€¢	Python tasks:
	â€¢	Fetch weather data from API
	â€¢	Upload CSV files to S3
	â€¢	Downstream transformations handled in Snowflake via dbt

â¸»

ğŸ“ˆ Example Analytical Use Cases
	â€¢	Order volume vs weather conditions
	â€¢	Impact of precipitation on delivery timing
	â€¢	Hourly order patterns correlated with temperature
	â€¢	Regional customer behavior analysis

â¸»

âœ… Key Engineering Practices Demonstrated
	â€¢	Separation of ingestion, storage, and transformation
	â€¢	Schema-on-read via dbt
	â€¢	Reproducible transformations
	â€¢	Clear data grain definition
	â€¢	Analytics-ready marts
	â€¢	Cloud-native architecture

â¸»

ğŸš€ What This Project Demonstrates

This project demonstrates practical skills expected from a Juniorâ€“Middle Data Engineer, including:
	â€¢	Designing end-to-end pipelines
	â€¢	Working with cloud storage and warehouses
	â€¢	Handling real-world timestamp challenges
	â€¢	Applying dbt modeling best practices
	â€¢	Orchestrating pipelines with Airflow
	â€¢	Writing production-quality SQL
